data:
  tokenizer_path: "models/gpt-clean-16000.json"
  train_path: "data/train_9.5M"
  eval_path: "data/select_0.5M"
  test_path: "data/dev_clean"
  seq_length: 128

model:
  type: "Llama" # or "GPT2"
  name: "SmolLlama-345M"
  hidden_size: 960
  intermediate_size: 2560
  n_layer: 32
  n_head: 15
  n_KV: 5

training:
  lr: 7e-4
  weight_decay: 5.0
  batch_size: 128
  num_epochs: 8
  gradient_accumulation_steps: 4
  warmup_steps: 600
  fp16: True
  torch_compile: True

distillation:
  alpha: 0.5
  temperature: 1.0

logging:
  wandb: True
  project: "babyllama"
  # Use {NAME} and {ID} to respectively insert the model name and unique ID
  output_path: "models/{NAME}/{ID}"
